{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3499ddd1",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "482d262d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from pprint import pprint\n",
    "\n",
    "nest_asyncio.apply()\n",
    "load_dotenv(find_dotenv())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd776c",
   "metadata": {},
   "source": [
    "# Messages and Chat History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d71a17",
   "metadata": {},
   "source": [
    "## Data Schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376d7fb8",
   "metadata": {},
   "source": [
    "- Agent run result `all_messages() -> list[ModelMessage]`\n",
    "- `ModelMessage` is an `typing.Annotated` of `ModelRequest` and `ModelResponse`\n",
    "- `ModelRequest` schema:\n",
    "> ```json\n",
    ">{\n",
    ">   parts: \"list[ModelRequestPart]\",\n",
    ">   instructions: \"str | None\",\n",
    ">   kind: \"Literal['request']\",\n",
    ">}\n",
    ">```\n",
    "\n",
    "- `ModelResponse` schema:\n",
    "> ```json\n",
    ">{\n",
    ">   parts: \"list[ModelResponsePart]\",\n",
    ">   model_name: \"str | None\",\n",
    ">   timestamp: \"datetime\",\n",
    ">   kind: \"Literal['response']\",\n",
    ">}\n",
    ">```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b554d1",
   "metadata": {},
   "source": [
    "- parts can be `SystemPromptPart`, `UserPromptPart`, `TextPart`, `ToolCallPart`...\n",
    "- `SystemPromptPart` schema:\n",
    ">```JSON\n",
    ">{\n",
    ">   content: \"str\",\n",
    ">   timestamp: \"datetime\",\n",
    ">   dynamic_ref: \"str\",\n",
    ">   part_kind: \"Literal['system-prompt']\"\n",
    ">}\n",
    ">```\n",
    "- `UserPromptPart` schema:\n",
    ">```JSON\n",
    ">{\n",
    ">   content: \"str | Sequence[UserContent]\",\n",
    ">   timestamp: \"datetime\",\n",
    ">   part_kind: \"Literal['user-prompt']\",\n",
    ">}\n",
    ">```\n",
    "- `TextPart` schema:\n",
    ">```JSON\n",
    ">{\n",
    ">   content: \"str\",\n",
    ">   timestamp: \"datetime\",\n",
    ">   part_kind: \"Literal['text']\",\n",
    ">}\n",
    ">```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99ee830",
   "metadata": {},
   "source": [
    "## Accessing Messages from Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29a0122c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent('google-gla:gemini-1.5-flash', system_prompt='Be a helpful assistant.')\n",
    "\n",
    "result = agent.run_sync('Tell me a joke.')\n",
    "\n",
    "pprint(type(result.all_messages()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ad0e95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't scientists trust atoms? \n",
      "\n",
      "Because they make up everything!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93a13702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ModelRequest(parts=[ SystemPromptPart(content='Be a helpful assistant.',\n",
      "                                        timestamp=datetime.datetime(2025, 4, 28, 6, 32, 41, 662047, tzinfo=datetime.timezone.utc),\n",
      "                                        dynamic_ref=None,\n",
      "                                        part_kind='system-prompt'),\n",
      "                       UserPromptPart(content='Tell me a joke.',\n",
      "                                      timestamp=datetime.datetime(2025, 4, 28, 6, 32, 41, 662047, tzinfo=datetime.timezone.utc),\n",
      "                                      part_kind='user-prompt')],\n",
      "               instructions=None,\n",
      "               kind='request'),\n",
      "  ModelResponse(parts=[ TextPart(content=\"Why don't scientists trust atoms? \\n\"\n",
      "                                         '\\n'\n",
      "                                         'Because they make up everything!\\n',\n",
      "                                 part_kind='text')],\n",
      "                model_name='gemini-1.5-flash',\n",
      "                timestamp=datetime.datetime(2025, 4, 28, 6, 32, 42, 413595, tzinfo=datetime.timezone.utc),\n",
      "                kind='response')]\n"
     ]
    }
   ],
   "source": [
    "# all messages from the run\n",
    "pprint(result.all_messages(), indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52ee2a2",
   "metadata": {},
   "source": [
    "## Accessing Messages from Streamed Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13413619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai.messages import ModelMessage\n",
    "\n",
    "async def streamed_results() -> list[ModelMessage]:\n",
    "    async with agent.run_stream(\"Tell me a joke.\") as result:\n",
    "        # incomplete messages before the stream finishes\n",
    "        pprint(result.all_messages())\n",
    "\n",
    "        async for text in result.stream_text():\n",
    "            print(text)\n",
    "\n",
    "        # complete messages once the stream finishes\n",
    "        return result.all_messages()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9cf8168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ModelRequest(parts=[SystemPromptPart(content='Be a helpful assistant.',\n",
      "                                      timestamp=datetime.datetime(2025, 4, 28, 6, 33, 6, 554441, tzinfo=datetime.timezone.utc),\n",
      "                                      dynamic_ref=None,\n",
      "                                      part_kind='system-prompt'),\n",
      "                     UserPromptPart(content='Tell me a joke.',\n",
      "                                    timestamp=datetime.datetime(2025, 4, 28, 6, 33, 6, 554441, tzinfo=datetime.timezone.utc),\n",
      "                                    part_kind='user-prompt')],\n",
      "              instructions=None,\n",
      "              kind='request')]\n",
      "Why don't scientists trust atoms? \n",
      "\n",
      "Because they make up everything!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "all_messages = asyncio.run(streamed_results())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e462162a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ModelRequest(parts=[SystemPromptPart(content='Be a helpful assistant.',\n",
      "                                      timestamp=datetime.datetime(2025, 4, 28, 6, 33, 6, 554441, tzinfo=datetime.timezone.utc),\n",
      "                                      dynamic_ref=None,\n",
      "                                      part_kind='system-prompt'),\n",
      "                     UserPromptPart(content='Tell me a joke.',\n",
      "                                    timestamp=datetime.datetime(2025, 4, 28, 6, 33, 6, 554441, tzinfo=datetime.timezone.utc),\n",
      "                                    part_kind='user-prompt')],\n",
      "              instructions=None,\n",
      "              kind='request'),\n",
      " ModelResponse(parts=[TextPart(content=\"Why don't scientists trust atoms? \\n\"\n",
      "                                       '\\n'\n",
      "                                       'Because they make up everything!\\n',\n",
      "                               part_kind='text')],\n",
      "               model_name='gemini-1.5-flash',\n",
      "               timestamp=datetime.datetime(2025, 4, 28, 6, 33, 8, 23804, tzinfo=datetime.timezone.utc),\n",
      "               kind='response')]\n"
     ]
    }
   ],
   "source": [
    "pprint(all_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd12365",
   "metadata": {},
   "source": [
    "## Using Messages as Input for Further Agent Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8970145",
   "metadata": {},
   "source": [
    "The primary use of message histories in PydanticAI is to maintain context across multiple agent runs.\n",
    "\n",
    "To use existing messages in a run, pass them to the `message_history` parameter of `Agent.run`, `Agent.run_sync` or `Agent.run_stream`.\n",
    "\n",
    "If `message_history` is set and not empty, a new system prompt is not generated — we assume the existing message history includes a system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff3c18e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't scientists trust atoms? \n",
      "\n",
      "Because they make up everything!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result1 = agent.run_sync(\"Tell me a joke.\")\n",
    "print(result1.output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b74655b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The joke plays on the double meaning of \"make up.\"  \n",
      "\n",
      "* **Literal meaning:** Atoms are the fundamental building blocks of matter, so they literally *make up* everything in the universe.\n",
      "\n",
      "* **Figurative meaning:**  \"Making something up\" means to invent or fabricate a lie.\n",
      "\n",
      "The humor comes from the unexpected shift between these two meanings.  The punchline suggests that because atoms are the basis of reality, they are untrustworthy because they could be \"making up\" anything.  It's a silly, absurd thought.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result2 = agent.run_sync(\"Explain?\", message_history=result1.new_messages())\n",
    "print(result2.output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5030b415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ModelRequest(parts=[ SystemPromptPart(content='Be a helpful assistant.',\n",
      "                                        timestamp=datetime.datetime(2025, 4, 24, 13, 56, 34, 329847, tzinfo=datetime.timezone.utc),\n",
      "                                        dynamic_ref=None,\n",
      "                                        part_kind='system-prompt'),\n",
      "                       UserPromptPart(content='Tell me a joke.',\n",
      "                                      timestamp=datetime.datetime(2025, 4, 24, 13, 56, 34, 329854, tzinfo=datetime.timezone.utc),\n",
      "                                      part_kind='user-prompt')],\n",
      "               instructions=None,\n",
      "               kind='request'),\n",
      "  ModelResponse(parts=[ TextPart(content=\"Why don't scientists trust atoms? \\n\"\n",
      "                                         '\\n'\n",
      "                                         'Because they make up everything!\\n',\n",
      "                                 part_kind='text')],\n",
      "                model_name='gemini-1.5-flash',\n",
      "                timestamp=datetime.datetime(2025, 4, 24, 13, 56, 35, 66663, tzinfo=datetime.timezone.utc),\n",
      "                kind='response'),\n",
      "  ModelRequest(parts=[ UserPromptPart(content='Explain?',\n",
      "                                      timestamp=datetime.datetime(2025, 4, 24, 13, 56, 58, 355578, tzinfo=datetime.timezone.utc),\n",
      "                                      part_kind='user-prompt')],\n",
      "               instructions=None,\n",
      "               kind='request'),\n",
      "  ModelResponse(parts=[ TextPart(content='The joke plays on the double meaning '\n",
      "                                         'of \"make up.\"  \\n'\n",
      "                                         '\\n'\n",
      "                                         '* **Literal meaning:** Atoms are the '\n",
      "                                         'fundamental building blocks of '\n",
      "                                         'matter, so they literally *make up* '\n",
      "                                         'everything in the universe.\\n'\n",
      "                                         '\\n'\n",
      "                                         '* **Figurative meaning:**  \"Making '\n",
      "                                         'something up\" means to invent or '\n",
      "                                         'fabricate a lie.\\n'\n",
      "                                         '\\n'\n",
      "                                         'The humor comes from the unexpected '\n",
      "                                         'shift between these two meanings.  '\n",
      "                                         'The punchline suggests that because '\n",
      "                                         'atoms are the basis of reality, they '\n",
      "                                         'are untrustworthy because they could '\n",
      "                                         'be \"making up\" anything.  It\\'s a '\n",
      "                                         'silly, absurd thought.\\n',\n",
      "                                 part_kind='text')],\n",
      "                model_name='gemini-1.5-flash',\n",
      "                timestamp=datetime.datetime(2025, 4, 24, 13, 56, 59, 724308, tzinfo=datetime.timezone.utc),\n",
      "                kind='response')]\n"
     ]
    }
   ],
   "source": [
    "pprint(result2.all_messages(), compact=True, indent=2, depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785e4d49",
   "metadata": {},
   "source": [
    "## Storing and Loading Messages (to JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437c4232",
   "metadata": {},
   "source": [
    "- When there's a need to store the messages history of an agent run on disk or in a database. This might be for evals, for sharing data between Python and JavaScript/TypeScript, or any number of other use cases.\n",
    "- PydanticAI uses a `TypeAdapter` to provide this capability. You can use `ModelMessagesTypeAdapter` or create a custom one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1172a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ModelRequest(parts=[SystemPromptPart(content='Be a helpful assistant.',\n",
      "                                      timestamp=datetime.datetime(2025, 4, 24, 14, 2, 49, 740980, tzinfo=datetime.timezone.utc),\n",
      "                                      dynamic_ref=None,\n",
      "                                      part_kind='system-prompt'),\n",
      "                     UserPromptPart(content='Tell me a joke.',\n",
      "                                    timestamp=datetime.datetime(2025, 4, 24, 14, 2, 49, 740984, tzinfo=datetime.timezone.utc),\n",
      "                                    part_kind='user-prompt')],\n",
      "              instructions=None,\n",
      "              kind='request'),\n",
      " ModelResponse(parts=[TextPart(content=\"Why don't scientists trust atoms? \\n\"\n",
      "                                       '\\n'\n",
      "                                       'Because they make up everything!\\n',\n",
      "                               part_kind='text')],\n",
      "               model_name='gemini-1.5-flash',\n",
      "               timestamp=datetime.datetime(2025, 4, 24, 14, 2, 50, 445901, tzinfo=datetime.timezone.utc),\n",
      "               kind='response')]\n"
     ]
    }
   ],
   "source": [
    "from pydantic_core import to_jsonable_python\n",
    "from pydantic_ai.messages import ModelMessagesTypeAdapter\n",
    "\n",
    "result1 = agent.run_sync(\"Tell me a joke.\")\n",
    "history_step_1 = result1.all_messages()\n",
    "\n",
    "pprint(history_step_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "125955b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'instructions': None,\n",
      "  'kind': 'request',\n",
      "  'parts': [{'content': 'Be a helpful assistant.',\n",
      "             'dynamic_ref': None,\n",
      "             'part_kind': 'system-prompt',\n",
      "             'timestamp': '2025-04-24T14:02:49.740980Z'},\n",
      "            {'content': 'Tell me a joke.',\n",
      "             'part_kind': 'user-prompt',\n",
      "             'timestamp': '2025-04-24T14:02:49.740984Z'}]},\n",
      " {'kind': 'response',\n",
      "  'model_name': 'gemini-1.5-flash',\n",
      "  'parts': [{'content': \"Why don't scientists trust atoms? \\n\"\n",
      "                        '\\n'\n",
      "                        'Because they make up everything!\\n',\n",
      "             'part_kind': 'text'}],\n",
      "  'timestamp': '2025-04-24T14:02:50.445901Z'}]\n"
     ]
    }
   ],
   "source": [
    "as_python_objects = to_jsonable_python(history_step_1)\n",
    "\n",
    "pprint(as_python_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94c8cd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ModelRequest(parts=[SystemPromptPart(content='Be a helpful assistant.',\n",
      "                                      timestamp=datetime.datetime(2025, 4, 24, 14, 2, 49, 740980, tzinfo=TzInfo(UTC)),\n",
      "                                      dynamic_ref=None,\n",
      "                                      part_kind='system-prompt'),\n",
      "                     UserPromptPart(content='Tell me a joke.',\n",
      "                                    timestamp=datetime.datetime(2025, 4, 24, 14, 2, 49, 740984, tzinfo=TzInfo(UTC)),\n",
      "                                    part_kind='user-prompt')],\n",
      "              instructions=None,\n",
      "              kind='request'),\n",
      " ModelResponse(parts=[TextPart(content=\"Why don't scientists trust atoms? \\n\"\n",
      "                                       '\\n'\n",
      "                                       'Because they make up everything!\\n',\n",
      "                               part_kind='text')],\n",
      "               model_name='gemini-1.5-flash',\n",
      "               timestamp=datetime.datetime(2025, 4, 24, 14, 2, 50, 445901, tzinfo=TzInfo(UTC)),\n",
      "               kind='response')]\n"
     ]
    }
   ],
   "source": [
    "same_history_as_step_1 = ModelMessagesTypeAdapter.validate_python(as_python_objects)\n",
    "pprint(same_history_as_step_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00cdfaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ModelRequest(parts=[SystemPromptPart(content='Be a helpful assistant.',\n",
      "                                      timestamp=datetime.datetime(2025, 4, 24, 14, 2, 49, 740980, tzinfo=TzInfo(UTC)),\n",
      "                                      dynamic_ref=None,\n",
      "                                      part_kind='system-prompt'),\n",
      "                     UserPromptPart(content='Tell me a joke.',\n",
      "                                    timestamp=datetime.datetime(2025, 4, 24, 14, 2, 49, 740984, tzinfo=TzInfo(UTC)),\n",
      "                                    part_kind='user-prompt')],\n",
      "              instructions=None,\n",
      "              kind='request'),\n",
      " ModelResponse(parts=[TextPart(content=\"Why don't scientists trust atoms? \\n\"\n",
      "                                       '\\n'\n",
      "                                       'Because they make up everything!\\n',\n",
      "                               part_kind='text')],\n",
      "               model_name='gemini-1.5-flash',\n",
      "               timestamp=datetime.datetime(2025, 4, 24, 14, 2, 50, 445901, tzinfo=TzInfo(UTC)),\n",
      "               kind='response'),\n",
      " ModelRequest(parts=[UserPromptPart(content='Tell me a different joke.',\n",
      "                                    timestamp=datetime.datetime(2025, 4, 24, 14, 4, 52, 468534, tzinfo=datetime.timezone.utc),\n",
      "                                    part_kind='user-prompt')],\n",
      "              instructions=None,\n",
      "              kind='request'),\n",
      " ModelResponse(parts=[TextPart(content='Parallel lines have so much in '\n",
      "                                       'common.  It’s a shame they’ll never '\n",
      "                                       'meet.\\n',\n",
      "                               part_kind='text')],\n",
      "               model_name='gemini-1.5-flash',\n",
      "               timestamp=datetime.datetime(2025, 4, 24, 14, 4, 53, 172131, tzinfo=datetime.timezone.utc),\n",
      "               kind='response')]\n"
     ]
    }
   ],
   "source": [
    "result2 = agent.run_sync(\n",
    "    \"Tell me a different joke.\", message_history=same_history_as_step_1\n",
    ")\n",
    "pprint(result2.all_messages())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0c8190",
   "metadata": {},
   "source": [
    "## Other Ways of Using Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857c50dd",
   "metadata": {},
   "source": [
    "- Since messages are defined by simple dataclasses, you can manually create and manipulate, e.g. for testing.\n",
    "- The message format is independent of the model used, so you can use messages in different agents, or the same agent with different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e060206f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't scientists trust atoms? \n",
      "\n",
      "Because they make up everything!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result1 = agent.run_sync(\"Tell me a joke.\")\n",
    "print(result1.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2475cf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The joke plays on the double meaning of \"make up.\"  \n",
      "\n",
      "* **Make up (meaning 1: invent):**  This implies that atoms are lying or not telling the truth.\n",
      "* **Make up (meaning 2: constitute):**  This refers to the scientific fact that atoms are the fundamental building blocks of all matter.\n",
      "\n",
      "The humor comes from the unexpected shift between the common understanding of \"making things up\" (like a story) and the scientific meaning.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result2 = agent.run_sync(\n",
    "    \"Explain?\",\n",
    "    model=\"google-gla:gemini-1.5-pro\",\n",
    "    message_history=result1.new_messages(),\n",
    ")\n",
    "print(result2.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0803264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ModelRequest(parts=[SystemPromptPart(content='Be a helpful assistant.',\n",
      "                                      timestamp=datetime.datetime(2025, 4, 24, 14, 15, 7, 768026, tzinfo=datetime.timezone.utc),\n",
      "                                      dynamic_ref=None,\n",
      "                                      part_kind='system-prompt'),\n",
      "                     UserPromptPart(content='Tell me a joke.',\n",
      "                                    timestamp=datetime.datetime(2025, 4, 24, 14, 15, 7, 768032, tzinfo=datetime.timezone.utc),\n",
      "                                    part_kind='user-prompt')],\n",
      "              instructions=None,\n",
      "              kind='request'),\n",
      " ModelResponse(parts=[TextPart(content=\"Why don't scientists trust atoms? \\n\"\n",
      "                                       '\\n'\n",
      "                                       'Because they make up everything!\\n',\n",
      "                               part_kind='text')],\n",
      "               model_name='gemini-1.5-flash',\n",
      "               timestamp=datetime.datetime(2025, 4, 24, 14, 15, 8, 562308, tzinfo=datetime.timezone.utc),\n",
      "               kind='response'),\n",
      " ModelRequest(parts=[UserPromptPart(content='Explain?',\n",
      "                                    timestamp=datetime.datetime(2025, 4, 24, 14, 15, 26, 406170, tzinfo=datetime.timezone.utc),\n",
      "                                    part_kind='user-prompt')],\n",
      "              instructions=None,\n",
      "              kind='request'),\n",
      " ModelResponse(parts=[TextPart(content='The joke plays on the double meaning '\n",
      "                                       'of \"make up.\"  \\n'\n",
      "                                       '\\n'\n",
      "                                       '* **Make up (meaning 1: invent):**  '\n",
      "                                       'This implies that atoms are lying or '\n",
      "                                       'not telling the truth.\\n'\n",
      "                                       '* **Make up (meaning 2: '\n",
      "                                       'constitute):**  This refers to the '\n",
      "                                       'scientific fact that atoms are the '\n",
      "                                       'fundamental building blocks of all '\n",
      "                                       'matter.\\n'\n",
      "                                       '\\n'\n",
      "                                       'The humor comes from the unexpected '\n",
      "                                       'shift between the common understanding '\n",
      "                                       'of \"making things up\" (like a story) '\n",
      "                                       'and the scientific meaning.\\n',\n",
      "                               part_kind='text')],\n",
      "               model_name='gemini-1.5-pro-002',\n",
      "               timestamp=datetime.datetime(2025, 4, 24, 14, 15, 28, 948148, tzinfo=datetime.timezone.utc),\n",
      "               kind='response')]\n"
     ]
    }
   ],
   "source": [
    "pprint(result2.all_messages())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
